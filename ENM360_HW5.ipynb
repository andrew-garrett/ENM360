{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ENM360 HW5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wLZVU0xadPuP",
        "2K8UrqEfutYu",
        "zrdtcLQR03-F"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLZVU0xadPuP"
      },
      "source": [
        "# **Problem 1 (MLP)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HG9HQJHpfHTn"
      },
      "source": [
        "First, we must import the necessary JAX library functions, namely random, vmap, grad, jit, ravel_pytree, and the Adam Optimizer.  In addition, we import the partial decorator and plotting tools used for visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpVcSOTddOOe"
      },
      "source": [
        "import jax.numpy as np\n",
        "from jax import random, vmap, grad, jit\n",
        "from jax.flatten_util import ravel_pytree\n",
        "from jax.experimental import optimizers\n",
        "\n",
        "import itertools\n",
        "from functools import partial\n",
        "from tqdm import trange\n",
        "import numpy.random as npr\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy.interpolate import griddata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "burZUdO6f7jb"
      },
      "source": [
        "Now, we can develop a class which can be used as a Multi-Layer Perceptron Network.  We use this as a feed-forward network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6OJyDTZdYgg"
      },
      "source": [
        "class MLP():\n",
        "  def __init__(self, X, y, layers, init_method = 'glorot', rng_key = random.PRNGKey(0)):\n",
        "    # Normalize data\n",
        "    self.Xmean, self.Xstd = X.mean(0), X.std(0)\n",
        "    self.Ymean, self.Ystd = y.mean(0), y.std(0)\n",
        "    X = (X - self.Xmean)/self.Xstd\n",
        "    y = (y - self.Ymean)/self.Ystd\n",
        "\n",
        "    # Store the normalized trainind data\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.layers = layers\n",
        "\n",
        "    # Use stax to set up network initialization and evaluation functions\n",
        "    self.net_init, self.net_apply = self.init_MLP(init_method)\n",
        "    \n",
        "    # Initialize parameters, not committing to a batch shape\n",
        "    self.net_params = self.net_init(rng_key, layers)\n",
        "                \n",
        "    # Use optimizers to set optimizer initialization and update functions\n",
        "    self.opt_init, \\\n",
        "    self.opt_update, \\\n",
        "    self.get_params = optimizers.sgd(1e-4)\n",
        "    self.opt_state = self.opt_init(self.net_params)\n",
        "\n",
        "    # Logger to monitor the loss function\n",
        "    self.loss_log = []\n",
        "    self.itercount = itertools.count()\n",
        "\n",
        "    # Logger to monitor error function\n",
        "\n",
        "    self.error_log = []\n",
        "\n",
        "  \"\"\"\n",
        "  First, we must initialize our MLP model.  We use a glorot initialization for \n",
        "  a uniform or a normal distribution for the weights. The init_W function \n",
        "  initializes our parameters (weights) according to each of the type of\n",
        "  intialization.  We then use the distribution of weights to initialize the\n",
        "  weights and biases for each layer of the MLP.\n",
        "  \"\"\"\n",
        "\n",
        "  def init_MLP(self, method = 'glorot'):\n",
        "    # Define init function\n",
        "    def _init(rng_key, layers):\n",
        "        # Define methods for initializing the weights\n",
        "        if method == 'glorot':\n",
        "          def init_W(rng_key, size):\n",
        "            in_dim = size[0]\n",
        "            out_dim = size[1]\n",
        "            glorot_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
        "            return glorot_stddev*random.normal(rng_key, (in_dim, out_dim))\n",
        "        elif method == 'random':\n",
        "          def init_W(rng_key, size):\n",
        "            in_dim = size[0]\n",
        "            out_dim = size[1]\n",
        "            return random.uniform(rng_key, (in_dim, out_dim))\n",
        "        # Perform initialization\n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers) \n",
        "        for l in range(0,num_layers-1):\n",
        "            rng_key, _ = random.split(rng_key)\n",
        "            W = init_W(rng_key, size=[layers[l], layers[l+1]])\n",
        "            b = np.zeros((1,layers[l+1]))\n",
        "            weights.append(W)\n",
        "            biases.append(b)  \n",
        "        params = weights, biases\n",
        "        return params\n",
        "    \n",
        "    \"\"\"\n",
        "    Next, we define the apply function, which creates the output through \n",
        "    the layers of the MLP, given a particular input.  The matrix H applies the\n",
        "    hyperbolic tangent activation function to the (H dot W) + b.  Once we get\n",
        "    through all layers, we can then take the last entry in the weights and\n",
        "    biases matrices and perform the same activation to output what the last\n",
        "    layer will yield.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define apply function\n",
        "    def _apply(params, input):\n",
        "        H = input\n",
        "        weights, biases = params\n",
        "        num_layers = len(self.layers)\n",
        "        for l in range(0,num_layers-2):\n",
        "            W = weights[l]\n",
        "            b = biases[l]\n",
        "            H = np.tanh(np.add(np.dot(H, W), b))\n",
        "        W = weights[-1]\n",
        "        b = biases[-1]\n",
        "        H = np.add(np.dot(H, W), b)\n",
        "        return H.flatten()\n",
        "    return _init, _apply\n",
        "\n",
        "  \"\"\"\n",
        "  Here, we use the standard formula for squared error to calculate the loss\n",
        "  for each element in a batch.  We take as inputs the current parameters and \n",
        "  batch of data, and output the loss of a single value.\n",
        "  \"\"\"\n",
        "\n",
        "  def per_example_loglikelihood(self, params, batch):\n",
        "    X, y = batch\n",
        "    y_pred = self.net_apply(params, X)\n",
        "    loss = (y - y_pred)**2\n",
        "    return loss\n",
        "\n",
        "  def per_example_logerror(self, params, batch):\n",
        "    X, y = batch\n",
        "    y_pred = self.net_apply(params, X)\n",
        "    loss = (y - y_pred)**2\n",
        "    return loss\n",
        "\n",
        "  \"\"\"\n",
        "  Now, we can leverage the per-batch loss calculation to determine the loss of \n",
        "  the entire batch as it passes through the MLP.  We take as inputs the current\n",
        "  parameters and batch of data, and output the loss of the entire batch.\n",
        "  \"\"\" \n",
        "\n",
        "  def loss(self, params, batch):\n",
        "    # Implementation #1\n",
        "    pe_loss = lambda x: self.per_example_loglikelihood(params, x)\n",
        "    loss = np.sum(vmap(pe_loss)(batch))\n",
        "    return loss\n",
        "\n",
        "  def error(self, params, batch):\n",
        "    X, y = batch\n",
        "    y_pred = self.net_apply(params, X)\n",
        "    error = np.linalg.norm(y - y_pred, 2)/np.linalg.norm(y, 2)\n",
        "    return error\n",
        "\n",
        "  \"\"\"\n",
        "  We can now update parameters based on the Adam Optimizer's update rule.  This\n",
        "  means that we are updating parameters with a variable learning rate.  We use\n",
        "  the partial decorator to indicate we'd like this function to utilize a\n",
        "  hardware accelerator to compile the code faster.  We take as inputs the \n",
        "  iteration number, the current optimized state of the network, and the current\n",
        "  batch number.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define a compiled update step\n",
        "  @partial(jit, static_argnums=(0,))\n",
        "  def step(self, i, opt_state, batch):\n",
        "      params = self.get_params(opt_state)\n",
        "      g = grad(self.loss)(params, batch)\n",
        "      return self.opt_update(i, g, opt_state)\n",
        "\n",
        "  \"\"\"\n",
        "  This function is used to create each batch from a given data set and output it\n",
        "  in a stream.  We take as inputs the dimension of the training data, the number\n",
        "  of batches, and the desired batch size.  We output the training data as the \n",
        "  desired number of batches whose dimensions match the desired size.\n",
        "  \"\"\"\n",
        "\n",
        "  def data_stream(self, n, num_batches, batch_size):\n",
        "    rng = npr.RandomState(0)\n",
        "    while True:\n",
        "      perm = rng.permutation(n)\n",
        "      for i in range(num_batches):\n",
        "        batch_idx = perm[i*batch_size:(i+1)*batch_size]\n",
        "        yield self.X[batch_idx, :], self.y[batch_idx]\n",
        "\n",
        "  \"\"\"\n",
        "  This is how we train the model.  Using the data_stream function to create\n",
        "  batches, we can iterate through each epoch to determine how the parameters \n",
        "  should be updated using our step function.  We take as inputs the number of \n",
        "  epochs and the desired batch size.  We output nothing, but after this function\n",
        "  has run, we have generated the optimized state for our MLP.\n",
        "  \"\"\"\n",
        "\n",
        "  def train(self, num_epochs = 100, batch_size = 64):   \n",
        "    n = self.X.shape[0]\n",
        "    num_complete_batches, leftover = divmod(n, batch_size)\n",
        "    num_batches = num_complete_batches + bool(leftover) \n",
        "    batches = self.data_stream(n, num_batches, batch_size)\n",
        "    pbar = trange(num_epochs)\n",
        "    for epoch in pbar:\n",
        "      for _ in range(num_batches):\n",
        "        batch = next(batches)\n",
        "        self.opt_state = self.step(next(self.itercount), self.opt_state, batch)\n",
        "      self.net_params = self.get_params(self.opt_state)\n",
        "      loss_value = self.loss(self.net_params, batch)\n",
        "      self.loss_log.append(loss_value)\n",
        "      # error_value = self.error(self.net_params, batch)\n",
        "      # self.error_log.append(error_value)\n",
        "      pbar.set_postfix({'Loss': loss_value})\n",
        "\n",
        "  \"\"\"\n",
        "  This function is used to generate the prediction our model produces given a \n",
        "  particular test data set.  We take as inputs the current parameters of our \n",
        "  model and the test data, and we output the prediction our model produces using\n",
        "  the inputs.\n",
        "  \"\"\"\n",
        "\n",
        "  def predict(self, params, X_star):\n",
        "    X_star = (X_star - self.Xmean)/self.Xstd\n",
        "    pred_fn = lambda x: self.net_apply(params, x)\n",
        "    y_pred = vmap(pred_fn)(X_star)\n",
        "    y_pred = y_pred*self.Ystd + self.Ymean\n",
        "    return y_pred\n",
        "\n",
        "  \"\"\"\n",
        "  This function is used to compute the activation of each layer in the the MLP\n",
        "  using the hyperbolic tangent.  This is very similar to how we initialize the \n",
        "  network.  We take as inputs the current parameters and a test data set, and we\n",
        "  output the matrix of each layer's output using the test data.\n",
        "  \"\"\"\n",
        "\n",
        "  def compute_activations(self, params, X_star):\n",
        "    X_star = (X_star - self.Xmean)/self.Xstd\n",
        "    def MLP_pass(params, input):\n",
        "      H = input\n",
        "      H_list = []\n",
        "      H_list.append(H)\n",
        "      weights, biases = params\n",
        "      num_layers = len(self.layers)\n",
        "      for l in range(0,num_layers-2):\n",
        "          W = weights[l]\n",
        "          b = biases[l]\n",
        "          H = np.tanh(np.add(np.dot(H, W), b))\n",
        "          H_list.append(H)\n",
        "      W = weights[-1]\n",
        "      b = biases[-1]\n",
        "      H = np.add(np.dot(H, W), b)\n",
        "      H_list.append(H)\n",
        "      return H_list\n",
        "    # Get predictions\n",
        "    pred_fn = lambda x: MLP_pass(params, x)\n",
        "    H_list = vmap(pred_fn)(X_star)\n",
        "    return H_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkYHAE-_q8dA"
      },
      "source": [
        "Here we have the function we'd like to develop a predictive model for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zkBtDnNdcGM"
      },
      "source": [
        "def f(x):\n",
        "  x1, x2 = x[0], x[1]\n",
        "  y = np.sqrt(x1**2 + x2**2)\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "307wRqMQrCbY"
      },
      "source": [
        "This is where we generate training data and test data.  The dimension of our data is 2 with lb and ub serving as how we can vectorize our input training/test data distributions to be sampled from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRx6bOo4dclm"
      },
      "source": [
        "rng_key = random.PRNGKey(0)\n",
        "\n",
        "d = 2\n",
        "lb = -2.0*np.ones(d)\n",
        "ub = 2.0*np.ones(d)\n",
        "n = 2000\n",
        "noise = 0.1\n",
        "\n",
        "# Create training data\n",
        "X = lb + (ub-lb)*random.uniform(rng_key, (n, d))\n",
        "y = vmap(f)(X)\n",
        "y = y + noise*y.std(0)*random.normal(rng_key, y.shape)\n",
        "\n",
        "# Create test data\n",
        "nn = 50\n",
        "xx = np.linspace(lb[0], ub[0], nn)\n",
        "yy = np.linspace(lb[1], ub[1], nn)\n",
        "XX, YY = np.meshgrid(xx, yy)\n",
        "X_star = np.concatenate([XX.flatten()[:,None], YY.flatten()[:,None]], axis = 1)\n",
        "y_star = vmap(f)(X_star)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zenv5henr5rG"
      },
      "source": [
        "Now we define the dimensions of each layer in the model, and using that, we can define the MLP model we'd like to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC3K-A46df2r"
      },
      "source": [
        "layers = [2, 32, 32, 32, 32, 1]\n",
        "init_method = 'glorot'\n",
        "model = MLP(X, y, layers, init_method, rng_key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqtPNCRysGr5"
      },
      "source": [
        "Now we train the model using the generated training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ur1vqdKZdiLZ"
      },
      "source": [
        "model.train(num_epochs = 2000, batch_size = 128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcGAlvPvsMjQ"
      },
      "source": [
        "After our model is trained, we can output the optimized parameters and the prediction of our test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "helrcmegdiYa"
      },
      "source": [
        "opt_params = model.net_params\n",
        "y_pred = model.predict(opt_params, X_star)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2szkJTW3sL4f"
      },
      "source": [
        "Finally, we can generate a visualization of our model's prediction using the optimal parameters and test data.  We first plot the surface that our model uses\n",
        "as a prediction for the function.  Then, we plot the Ground Truth as a function of the predicted values of our model (to gauge model performance in testing).  Lastly, we plot the log(loss) of our model over the course of training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rdsIJwJdpSt"
      },
      "source": [
        "Yplot = griddata(X_star, y_pred.flatten(), (XX, YY), method='cubic')\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot(X[:,0], X[:,1], y, 'r.', ms = 6, alpha = 0.5)\n",
        "ax.plot_surface(XX, YY, Yplot, alpha = 0.8)\n",
        "# Hide grid lines\n",
        "ax.grid(False)\n",
        "# Hide axes ticks\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "ax.set_zticks([])\n",
        "ax.set_xlabel('$x_1$')\n",
        "ax.set_ylabel('$x_2$')\n",
        "ax.set_zlabel('$y$')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(y_pred, y_star, 'r.', ms = 8, alpha = 0.5)\n",
        "plt.plot(y_star, y_star, 'k--', lw = 3, alpha = 0.5)\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Ground truth')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(model.loss_log)\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9aWYokLteov"
      },
      "source": [
        "Now, we can take all the data for our model and compute the activations of the optimized model.  We can also compute the gradient of the loss with respect to the parameters and input data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsSvKxMGdtVc"
      },
      "source": [
        "full_batch = model.X, model.y\n",
        "activations = model.compute_activations(opt_params, X_star)\n",
        "weight_grads, bias_grads = grad(model.loss)(opt_params, full_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYNxhq35t8yB"
      },
      "source": [
        "Now, we can estimate the probability density function for the activation layers and the probability density function for the gradient of the weights.  This is done for each of the layers in our MLP.  Being honest, I am a bit lost on what exactly this cell is doing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ivXfYubdt3s"
      },
      "source": [
        "from scipy.stats import kde\n",
        "\n",
        "def kde1d(x):\n",
        "  nn = 1000\n",
        "  xi = np.linspace(x.min(), x.max(), nn)\n",
        "  k = kde.gaussian_kde(x)\n",
        "  zi = k(xi)\n",
        "  return xi,zi\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(1,2,1)\n",
        "for i in range(1,len(activations)-1):\n",
        "  x, z = kde1d(activations[i].flatten())\n",
        "  plt.plot(x, z, label = 'Layer %d' % (i))\n",
        "plt.legend()\n",
        "plt.xlabel('Activation')\n",
        "plt.ylabel('Density')\n",
        "plt.tight_layout()\n",
        "plt.subplot(1,2,2)\n",
        "for i in range(1,len(weight_grads)-1):\n",
        "  x, z = kde1d(weight_grads[i].flatten())\n",
        "  plt.plot(x, z, label = 'Layer %d' % (i))\n",
        "plt.legend()\n",
        "plt.xlabel('Gradient')\n",
        "plt.ylabel('Density')\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K8UrqEfutYu"
      },
      "source": [
        "# **Problem 1 (CNN)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf4Y3-zwvbfo"
      },
      "source": [
        "First, we again import the necessary tools in the JAX library.  Notably, we also include the useful built in functions specifically for Convolutional Neural Networks.  We also import pytorch (for data acquisition), the Adam Optimizer, necessary plotting utilities, and the partial decorator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aMEMviAuyHi"
      },
      "source": [
        "import jax.numpy as np\n",
        "from jax import random\n",
        "from jax.experimental import stax\n",
        "from jax.experimental.stax import BatchNorm, Conv, Dense, Flatten, Relu, Softmax\n",
        "from jax.experimental import optimizers\n",
        "from jax import jit, grad\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import numpy as onp\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "from functools import partial\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJA-9GMlv_kp"
      },
      "source": [
        "Here we first define a function whose input is the labels and number of classes in our CNN, and whose output is a one hot encoding of the labels.  The second function is the proper initialization for a convolutional network with 5 Convolutions, each followed by a normalization operation, and a Rectified Linear Unit Activation function.  We have a densely connected layer which computes the class scores and then apply a Softmax activation function to generate our output.  The output will be a classifcation of the input according to the classes we have given.  This is all done in a serial manner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSYLxDgjuyZd"
      },
      "source": [
        "def one_hot(labels, num_classes, dtype=np.float32):\n",
        "  return np.array(labels[:, None] == np.arange(num_classes), dtype)\n",
        "\n",
        "def mnist_simple_cnn(num_classes):\n",
        "  init_fun, conv_net = stax.serial(Conv(out_chan=32, filter_shape=(5, 5), strides=(2, 2), padding=\"SAME\"),\n",
        "                                   BatchNorm(), Relu,\n",
        "                                   Conv(out_chan=32, filter_shape=(5, 5), strides=(2, 2), padding=\"SAME\"),\n",
        "                                   BatchNorm(), Relu,\n",
        "                                   Conv(out_chan=10, filter_shape=(3, 3), strides=(2, 2), padding=\"SAME\"),\n",
        "                                   BatchNorm(), Relu,\n",
        "                                   Conv(out_chan=10, filter_shape=(3, 3), strides=(2, 2), padding=\"SAME\"), \n",
        "                                   BatchNorm(), Relu,\n",
        "                                   Flatten,\n",
        "                                   Dense(num_classes),\n",
        "                                   Softmax)\n",
        "  return init_fun, conv_net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syvM9fLv9jAt"
      },
      "source": [
        "Now we define the class which will be used to generate and train a Convolutional Neural Network which can classify input data into user given classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbp6Vc71uync"
      },
      "source": [
        "class CNNclassifier:\n",
        "\n",
        "  \"\"\"\n",
        "  First, we must intialize the network.  We need\n",
        "  to keep track of the number of classes.  We use the\n",
        "  above functions to the initialize the network.  We then\n",
        "  initialize the parameters for the network and instantiate\n",
        "  the Adam optimizer to be used for updating parameters.\n",
        "  Lastly, we set up a counter for iterations, and arrays to\n",
        "  track accuracy and loss.\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize the class\n",
        "  def __init__(self, num_classes, rng_key):\n",
        "    # Store number of classes\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "    # Use stax to set up network initialization and evaluation functions\n",
        "    self.net_init, self.net_apply = mnist_simple_cnn(self.num_classes)\n",
        "    \n",
        "    # Initialize parameters, not committing to a batch shape\n",
        "    _, self.net_params = self.net_init(rng_key, (-1, 1, 28, 28))\n",
        "                \n",
        "    # Use optimizers to set optimizer initialization and update functions\n",
        "    self.opt_init, \\\n",
        "    self.opt_update, \\\n",
        "    self.get_params = optimizers.adam(optimizers.exponential_decay(1e-3, \n",
        "                                                                    decay_steps=100, \n",
        "                                                                    decay_rate=0.99))\n",
        "    self.opt_state = self.opt_init(self.net_params)\n",
        "\n",
        "    # Logger\n",
        "    self.itercount = itertools.count()\n",
        "    self.log_acc_train = []\n",
        "    self.log_acc_test = [] \n",
        "    self.train_loss = []\n",
        "  \n",
        "  \"\"\"\n",
        "  We take as input the parameters and the current batch\n",
        "  of data, and output the mean squared error loss.  \n",
        "  \"\"\"\n",
        "\n",
        "  # Define a simple mean squared-error loss\n",
        "  def loss(self, params, batch):\n",
        "      inputs, targets = batch\n",
        "      predictions = self.net_apply(params, inputs)\n",
        "      loss = -np.sum(targets*np.log(predictions + 1e-8))\n",
        "      return loss\n",
        "    \n",
        "  \"\"\"\n",
        "  We take as input the iteration number, the current\n",
        "  state of the Adam optimizer, and the current batch\n",
        "  of data.  We output the updated state of the Adam\n",
        "  optimizer.  The partial decorator indicates we'd\n",
        "  like to prioritize the speed of this function's \n",
        "  compilation and runtime using software and hardware\n",
        "  acceleration.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define a compiled update step\n",
        "  @partial(jit, static_argnums=(0,))\n",
        "  def step(self, i, opt_state, batch):\n",
        "      params = self.get_params(opt_state)\n",
        "      g = grad(self.loss)(params, batch)\n",
        "      return self.opt_update(i, g, opt_state)\n",
        "\n",
        "  \"\"\"\n",
        "  Now, we can compute the accuracy of our model\n",
        "  on a given dataset with it's curent parameters.\n",
        "  This is done using the a one-hot comparison\n",
        "  for the each batch in a dataset/dataloader.  I'm\n",
        "  not sure but I would guess that a dataloader functions\n",
        "  similar to a stream in java.\n",
        "  \"\"\"\n",
        "\n",
        "  def accuracy(self, params, data_loader):\n",
        "    \"\"\" Compute the accuracy for a provided dataloader \"\"\"\n",
        "    acc_total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "        inputs = np.array(inputs)\n",
        "        targets = one_hot(np.array(targets), self.num_classes)\n",
        "        target_labels = np.argmax(targets, axis=1)\n",
        "        predicted_labels = np.argmax(self.net_apply(params, inputs), axis=1)\n",
        "        acc_total += np.sum(predicted_labels == target_labels)\n",
        "    return acc_total/len(data_loader.dataset)\n",
        "  \n",
        "  \"\"\"\n",
        "  Here we train our CNN.  This is done by\n",
        "  performing our accelerated step function\n",
        "  for each batch in a dataset, for a given\n",
        "  number of epochs.  We also compute the training\n",
        "  and test accuracy for classification in \n",
        "  each epoch.  We take as inputs the streams of \n",
        "  training data and test data, as well as the \n",
        "  number of epochs.  At the termination of this\n",
        "  function, our model will be trained and tested,\n",
        "  with data characterizing its accuracy, and the set\n",
        "  of optimal parameters for the CNN.\n",
        "  \"\"\"\n",
        "\n",
        "  # Optimize parameters in a loop\n",
        "  def train(self, train_loader, test_loader, num_epochs = 1000):\n",
        "    for epoch in range(num_epochs):\n",
        "      start_time = time.time()\n",
        "      # Run epoch\n",
        "      for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "          batch = np.array(inputs), one_hot(np.array(targets), self.num_classes)\n",
        "          self.opt_state = self.step(next(self.itercount), self.opt_state, batch)\n",
        "      epoch_time = time.time() - start_time\n",
        "      # Compute training and validation accuracy\n",
        "      self.net_params = self.get_params(self.opt_state)  \n",
        "      loss = self.loss(self.net_params, batch)\n",
        "      train_acc = self.accuracy(self.net_params, train_loader)\n",
        "      test_acc = self.accuracy(self.net_params, test_loader)\n",
        "      self.train_loss.append(loss)\n",
        "      self.log_acc_train.append(train_acc)\n",
        "      self.log_acc_test.append(test_acc)\n",
        "      print(\"Epoch {} | Time: {:0.2f} | Train Acc.: {:0.3f}% | Test Acc.: {:0.3f}%\".format(epoch+1, epoch_time,\n",
        "                                                                  train_acc, test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE9HkcB3BZg4"
      },
      "source": [
        "Here we generate the training data stream and test data stream.  I'm not really sure how this works, but I think it is more important that I understand what it is doing.  This is essentially how we take batches sequentially from input datasets and can pull from them iteratively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUNem4D3uy1l"
      },
      "source": [
        "# Set the PyTorch Data Loader for the training & test set\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQuQT7itHb8P"
      },
      "source": [
        "Here, we define the number of classes for classification and initialize the Convolutional Neural Network accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72VUGMCDuzE7"
      },
      "source": [
        "num_classes = 10\n",
        "init_key = random.PRNGKey(0)\n",
        "model = CNNclassifier(num_classes, init_key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSvgsXibH6-u"
      },
      "source": [
        "Here, we train our CNN using the training data stream and test data stream.  We dont need many epochs for this task because the the data streams have a large amount of data.  It is possible that training for the same number of epochs as some of our MLP's (i.e. on the order of 1000 epochs), that overfitting could occur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aoi2NjlJuzUL"
      },
      "source": [
        "model.train(train_loader, test_loader, num_epochs = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db1XRxD5ImW_"
      },
      "source": [
        "Now that we have trained the CNN, we can visualize the log(loss) over the number of epochs, as well as the model accuracy over the number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ESi3XnSuzjM"
      },
      "source": [
        "plt.figure()\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(model.train_loss)\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training loss')\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(model.log_acc_train, label = 'Training')\n",
        "plt.plot(model.log_acc_test, label = 'Testing')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLSdUA00JLXE"
      },
      "source": [
        "We can then visualize the model applying a classification to the data in the test stream.  In this case, we are attempting to classify handwritten numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAMBQ8JguzxQ"
      },
      "source": [
        "# Visualize some predictions\n",
        "plt.figure(figsize = (8,8))\n",
        "for batch_idx, (image, label) in enumerate(test_loader): \n",
        "  # Perform predictions\n",
        "  opt_params = model.net_params\n",
        "  predicted_labels = np.argmax(model.net_apply(opt_params, np.array(image)), axis=-1)\n",
        "  plt.imshow(image[-1].reshape(28,28))\n",
        "  plt.title('This is a %d' % (predicted_labels[-1]))\n",
        "  plt.pause(0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73pIPNIpKJDJ"
      },
      "source": [
        "# **Problem 1 (RNN)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tal3gc4IKsxG"
      },
      "source": [
        "Again, we first import the necessary JAX library functions, the Adam optimizer, partial decorators, and the appropriate graphing tools.  These will be used to generate, train, and test a Recurrent Neural Network, whose output accuracy/error can be visualized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18gGJNQvKN24"
      },
      "source": [
        "import jax.numpy as np\n",
        "from jax import random, vmap, grad, jit\n",
        "from jax.experimental import optimizers\n",
        "from jax.ops import index_update, index\n",
        "\n",
        "import itertools\n",
        "from functools import partial\n",
        "from tqdm import trange\n",
        "import numpy.random as npr\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy.interpolate import griddata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDWp0CyeLOjC"
      },
      "source": [
        "We define a function to generate lags.  We take as input a dataset and a lag value.  We output a 3D array of the inputs and empirical outputs X,Y whose index in a particular layer is \"lagged\" according to the lag value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj9hXmjjKODp"
      },
      "source": [
        "def create_lags(data, L):\n",
        "    N = data.shape[0] - L\n",
        "    D = data.shape[1]\n",
        "    X = np.zeros((L, N, D))\n",
        "    Y = np.zeros((N, D))\n",
        "    for i in range(0,N):\n",
        "        X = index_update(X, index[:,i,:], data[i:(i+L), :])\n",
        "        Y = index_update(Y, index[i,:], data[i+L, :])\n",
        "    return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63i0w0IgMTDt"
      },
      "source": [
        "Here we develop the class that can generate, train, and test a Reccuring Neural Network, whose accuracy can be quantified accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ89q_hyKOVC"
      },
      "source": [
        "class RNN():\n",
        "\n",
        "  \"\"\"\n",
        "  First, we must initialize the RNN.  We take as inputs\n",
        "  the dataset, the number of lags, and the number of hidden\n",
        "  layers.  We first normalize the dataset according to its\n",
        "  mean and standard deviation.  We then create our lagged\n",
        "  normalized dataset using the above function.  We initialize\n",
        "  the network, apply rule, and parameters.  Then, we designate\n",
        "  the Adam optimizer and its state, as well as an array to track\n",
        "  the loss and a counter variable.\n",
        "  \"\"\"\n",
        "  def __init__(self, dataset, num_lags, hidden_dim, rng_key = random.PRNGKey(0)):\n",
        "    # Normalize across data-points dimension\n",
        "    self.mean, self.std = dataset.mean(0), dataset.std(0)\n",
        "    dataset = (dataset - self.mean)/self.std\n",
        "\n",
        "    # Create the lagged normalized trainind data\n",
        "    # X: L x N x D\n",
        "    # Y: N x D\n",
        "    self.X, self.Y = create_lags(dataset, num_lags)\n",
        "    self.X_dim = self.X.shape[-1]\n",
        "    self.Y_dim = self.Y.shape[-1]\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.num_lags = num_lags\n",
        "\n",
        "    # Initialization and evaluation functions\n",
        "    self.net_init, self.net_apply = self.init_RNN()\n",
        "    \n",
        "    # Initialize parameters, not committing to a batch shape\n",
        "    self.net_params = self.net_init(rng_key)\n",
        "                \n",
        "    # Use optimizers to set optimizer initialization and update functions\n",
        "    self.opt_init, \\\n",
        "    self.opt_update, \\\n",
        "    self.get_params = optimizers.adam(1e-3)\n",
        "    self.opt_state = self.opt_init(self.net_params)\n",
        "\n",
        "    # Logger to monitor the loss function\n",
        "    self.loss_log = []\n",
        "    self.itercount = itertools.count()\n",
        "\n",
        "  \"\"\"\n",
        "  Here we initialize the RNN.  We use the glorot\n",
        "  initialization according the number of hidden\n",
        "  layers and the dimension of the lagged normalized\n",
        "  input data X.  We also initialize the biases, weights,\n",
        "  and the output layer.  We output all of these.  We\n",
        "  also define the apply function for our RNN.  This\n",
        "  takes as input the current parameters and some input\n",
        "  data.  We apply a hyperbolic activation function to\n",
        "  (H*W + input*U + b).  This performs the application of \n",
        "  the RNN to input data, according to the activation\n",
        "  functions.\n",
        "  \"\"\"\n",
        "\n",
        "  def init_RNN(self):\n",
        "    # Define init function\n",
        "    def _init(rng_key):\n",
        "        # Define methods for initializing the weights\n",
        "        def glorot_normal(rng_key, size):\n",
        "          in_dim = size[0]\n",
        "          out_dim = size[1]\n",
        "          glorot_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
        "          return glorot_stddev*random.normal(rng_key, (in_dim, out_dim))\n",
        "        # Inputs\n",
        "        U = glorot_normal(rng_key, (self.X_dim, self.hidden_dim))\n",
        "        b = np.zeros(self.hidden_dim)\n",
        "        # Transition dynamics\n",
        "        W = np.eye(self.hidden_dim)\n",
        "        # Outputs\n",
        "        V = glorot_normal(rng_key, (self.hidden_dim, self.Y_dim))\n",
        "        c = np.zeros(self.Y_dim)\n",
        "        return (U, b, W, V, c)\n",
        "    # Define apply function\n",
        "    def _apply(params, input):\n",
        "        U, b, W, V, c = params\n",
        "        H = np.zeros((input.shape[1], self.hidden_dim))\n",
        "        for i in range(self.num_lags):\n",
        "            H = np.tanh(np.matmul(H, W) + np.matmul(input[i,:,:], U) + b)       \n",
        "        H = np.matmul(H, V) + c\n",
        "        return H\n",
        "    return _init, _apply\n",
        "\n",
        "  \"\"\"\n",
        "  Here we calculate the mean squared error loss on an input\n",
        "  of the current parameters and the current batch of data.\n",
        "  \"\"\"\n",
        "\n",
        "  def loss(self, params, batch):\n",
        "    X, y = batch\n",
        "    y_pred = self.net_apply(params, X)\n",
        "    loss = np.mean((y - y_pred)**2)\n",
        "    return loss\n",
        "\n",
        "  \"\"\"\n",
        "  Here we definte the step function of our optimizer.\n",
        "  We take as inputs the iteration number, current Adam\n",
        "  optimizer state, and current batch of data.  We then\n",
        "  output the updated state of the Adam optimizer using\n",
        "  the given data.  We use the partial decorator to indicate\n",
        "  that we will be using this function many times in rapid\n",
        "  succession, so it is best to prioritize the speed of its\n",
        "  compilation and processing using hardware and software\n",
        "  accleration.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define a compiled update step\n",
        "  @partial(jit, static_argnums=(0,))\n",
        "  def step(self, i, opt_state, batch):\n",
        "      params = self.get_params(opt_state)\n",
        "      g = grad(self.loss)(params, batch)\n",
        "      return self.opt_update(i, g, opt_state)\n",
        "\n",
        "  \"\"\"\n",
        "  This function is used to create each batch from a given data set and output it\n",
        "  in a stream.  We take as inputs the dimension of the training data, the number\n",
        "  of batches, and the desired batch size.  We output the training data as the \n",
        "  desired number of batches whose dimensions match the desired size.\n",
        "  \"\"\"\n",
        "\n",
        "  def data_stream(self, n, num_batches, batch_size):\n",
        "    rng = npr.RandomState(0)\n",
        "    while True:\n",
        "      perm = rng.permutation(n)\n",
        "      for i in range(num_batches):\n",
        "        batch_idx = perm[i*batch_size:(i+1)*batch_size]\n",
        "        yield self.X[:, batch_idx, :], self.Y[batch_idx, :]\n",
        "\n",
        "  \"\"\"\n",
        "  This is how we train the RNN.  We perform the accelerated\n",
        "  update rule on each batch in the training data stream, for\n",
        "  a given number of epochs.  We also update our parameters and\n",
        "  loss at each epoch.  We take as input te \n",
        "  \"\"\"\n",
        "\n",
        "  def train(self, num_epochs = 100, batch_size = 64):   \n",
        "    n = self.X.shape[1]\n",
        "    num_complete_batches, leftover = divmod(n, batch_size)\n",
        "    num_batches = num_complete_batches + bool(leftover) \n",
        "    batches = self.data_stream(n, num_batches, batch_size)\n",
        "    pbar = trange(num_epochs)\n",
        "    for epoch in pbar:\n",
        "      for _ in range(num_batches):\n",
        "        batch = next(batches)\n",
        "        self.opt_state = self.step(next(self.itercount), self.opt_state, batch)\n",
        "      self.net_params = self.get_params(self.opt_state)\n",
        "      loss_value = self.loss(self.net_params, batch)\n",
        "      self.loss_log.append(loss_value)\n",
        "      pbar.set_postfix({'Loss': loss_value})\n",
        "\n",
        "  \"\"\"\n",
        "  Finally, we have a function that can take in the current\n",
        "  parameters and some input test data to make a prediction\n",
        "  using the RNN.  We again use the partial decorator to speed\n",
        "  up the testing of our RNN.\n",
        "  \"\"\"\n",
        "\n",
        "  @partial(jit, static_argnums=(0,))\n",
        "  def predict(self, params, inputs):\n",
        "    Y_pred = self.net_apply(params, inputs)\n",
        "    return Y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW10qNcvRYdz"
      },
      "source": [
        "Here we define a function that we'd like to model using an RNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9acLmwuZKOja"
      },
      "source": [
        "def f(x):\n",
        "    f = np.sin(np.pi*t)\n",
        "    return f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KgGtyj_Rg7z"
      },
      "source": [
        "Now, we can generate the training and test data sets using linearly spaced inputs and the function output as validation.  We use 2/3 of the data for\n",
        "training, which is a reasonable split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a88BKW1sKOzA"
      },
      "source": [
        "rng_key = random.PRNGKey(0)\n",
        "noise = 0.0\n",
        "\n",
        "t = np.arange(0,10,0.1)[:,None]\n",
        "dataset = f(t)\n",
        "dataset = dataset + dataset.std(0)*noise*random.normal(rng_key, dataset.shape)\n",
        "\n",
        "# Use 2/3 of all data as training Data\n",
        "train_size = int(len(dataset) * (2.0/3.0))\n",
        "train_data = dataset[0:train_size,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtkE1C-sR4qA"
      },
      "source": [
        "Here, we create our Recurrent Neural Network with 5 lags and 4 hidden layers, with the generated training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrJoZD5IKO_T"
      },
      "source": [
        "# Model creation\n",
        "num_lags = 5\n",
        "hidden_dim = 4\n",
        "model = RNN(train_data, num_lags, hidden_dim, rng_key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqnBSCfASE7y"
      },
      "source": [
        "We train the model on 10,000 epochs and designate the batch size to be 128."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBF06xHsKPI6"
      },
      "source": [
        "model.train(num_epochs = 10000, batch_size = 128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi_J4AGVSOg_"
      },
      "source": [
        "Now that our model is trained according to the generated training data, we can extract the optimized parameters.  We can then make a prediction of the normalized data according the parameters and the number of lags.  We output the de-normalized predictions, as well as teh L2 error for our testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mLw_S0xKPR1"
      },
      "source": [
        "opt_params = model.net_params\n",
        "# One-step ahead prediction (normalized)\n",
        "N, D = dataset.shape\n",
        "pred = np.zeros((N-num_lags, D))\n",
        "X_tmp =  model.X[:,0:1,:]\n",
        "for i in trange(N-num_lags):\n",
        "    pred = index_update(pred, index[i:i+1], model.net_apply(opt_params, X_tmp))\n",
        "    X_tmp = index_update(X_tmp, index[:-1,:,:], X_tmp[1:,:,:])\n",
        "    X_tmp = index_update(X_tmp, index[-1,:,:], pred[i])\n",
        "# De-normalize predictions\n",
        "pred = pred*model.std + model.mean\n",
        "error = np.linalg.norm(dataset[num_lags:] - pred, 2)/np.linalg.norm(dataset[num_lags:], 2)\n",
        "print('Relative L2 prediction error: %e' % (error))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CE09yhGcSyi7"
      },
      "source": [
        "Finally, we can visualize the how closely our trained model can predict the output of the desired function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAOHJQ8TKPbF"
      },
      "source": [
        "plt.figure(1)\n",
        "plt.plot(dataset[num_lags:], 'b-', linewidth = 2, label = \"Exact\")\n",
        "plt.plot(pred, 'r--', linewidth = 3, label = \"Prediction\")\n",
        "# plt.plot(X.shape[1]*np.ones((2,1)), np.linspace(-1.75,1.75,2), 'k--', linewidth=2)\n",
        "plt.axvline(train_size)\n",
        "plt.axis('tight')\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$y_t$')\n",
        "plt.legend(loc='lower left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBqzZ3XUT7pY"
      },
      "source": [
        "# **Problem 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69dsZoWYT_bI"
      },
      "source": [
        "We'd like to generate a Deep Neural Network to model a given function f(x), such that there are 3 hidden layers, 50 neurons per layer, and a hyperbolic tangent activation function.  We will use the framework given in class for an RNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtifmbD2U-Lh"
      },
      "source": [
        "import jax.numpy as np\n",
        "from jax import random, vmap, grad, jit\n",
        "from jax.flatten_util import ravel_pytree\n",
        "from jax.experimental import optimizers\n",
        "\n",
        "import itertools\n",
        "from functools import partial\n",
        "from tqdm import trange\n",
        "import numpy.random as npr\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy.interpolate import griddata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYN7NrqAVKBn"
      },
      "source": [
        "We will use the code from part 1 on number 1 for the MLP class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMFC9fN8WGHO"
      },
      "source": [
        "def f(x):\n",
        "  x1, x2 = x[0], x[1]\n",
        "  f = np.sin(2*np.pi*x1) + np.sin(3*np.pi*x2) + np.sin(4*np.pi*x2)\n",
        "  return f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qif91gjdW-9f"
      },
      "source": [
        "rng_key = random.PRNGKey(0)\n",
        "\n",
        "d = 2\n",
        "lb = 3.0*np.ones(d)\n",
        "ub = 4.0*np.ones(d)\n",
        "n = 2000\n",
        "noise = 0.1\n",
        "\n",
        "# Create training data\n",
        "X = lb + (ub-lb)*random.uniform(rng_key, (n, d))\n",
        "y = vmap(f)(X)\n",
        "y = y + noise*y.std(0)*random.normal(rng_key, y.shape)\n",
        "\n",
        "# Create test data\n",
        "nn = 50\n",
        "xx = np.linspace(lb[0], ub[0], nn)\n",
        "yy = np.linspace(lb[1], ub[1], nn)\n",
        "XX, YY = np.meshgrid(xx, yy)\n",
        "X_star = np.concatenate([XX.flatten()[:,None], YY.flatten()[:,None]], axis = 1)\n",
        "y_star = vmap(f)(X_star)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeggr9ixX2dy"
      },
      "source": [
        "# Model Creation\n",
        "layers = [2, 50, 50, 50, 1]\n",
        "init_method = 'glorot'\n",
        "model = MLP(X, y, layers, init_method, rng_key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN0Z0JgNZRJt"
      },
      "source": [
        "model.train(num_epochs = 500, batch_size = 128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_30aUsPZiir",
        "outputId": "ddcd946d-97bb-4fc5-ee4f-c9663e0202c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "opt_params = model.net_params\n",
        "y_pred = model.predict(opt_params, X_star)\n",
        "\n",
        "error = np.linalg.norm(y_star - y_pred, 2)/np.linalg.norm(y_star, 2)\n",
        "print('Relative L2 prediction error: %e' % (error))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Relative L2 prediction error: 4.929670e+01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vauVglW7aM9X"
      },
      "source": [
        "Yplot = griddata(X_star, y_pred.flatten(), (XX, YY), method='cubic')\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot(X[:,0], X[:,1], y, 'r.', ms = 6, alpha = 0.5)\n",
        "ax.plot_surface(XX, YY, Yplot, alpha = 0.8)\n",
        "# Hide grid lines\n",
        "ax.grid(False)\n",
        "# Hide axes ticks\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "ax.set_zticks([])\n",
        "ax.set_xlabel('$x_1$')\n",
        "ax.set_ylabel('$x_2$')\n",
        "ax.set_zlabel('$y$')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(y_pred, y_star, 'r.', ms = 8, alpha = 0.5)\n",
        "plt.plot(y_star, y_star, 'k--', lw = 3, alpha = 0.5)\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Ground truth')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(model.loss_log)\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBLj3RMBs9nb"
      },
      "source": [
        "rnge = np.array([50, 100, 250, 500, 1000, 2000, 5000, 10000])\n",
        "error = []\n",
        "\n",
        "for n in rnge:\n",
        "  rng_key = random.PRNGKey(0)\n",
        "\n",
        "  d = 2\n",
        "  lb = 3.0*np.ones(d)\n",
        "  ub = 4.0*np.ones(d)\n",
        "  noise = 0.1\n",
        "\n",
        "  # Create training data\n",
        "  X = lb + (ub-lb)*random.uniform(rng_key, (n, d))\n",
        "  y = vmap(f)(X)\n",
        "  y = y + noise*y.std(0)*random.normal(rng_key, y.shape)\n",
        "\n",
        "  # Create test data\n",
        "  nn = 50\n",
        "  xx = np.linspace(lb[0], ub[0], nn)\n",
        "  yy = np.linspace(lb[1], ub[1], nn)\n",
        "  XX, YY = np.meshgrid(xx, yy)\n",
        "  X_star = np.concatenate([XX.flatten()[:,None], YY.flatten()[:,None]], axis = 1)\n",
        "  y_star = vmap(f)(X_star)\n",
        "\n",
        "  # Model Creation\n",
        "  layers = [2, 50, 50, 50, 1]\n",
        "  init_method = 'glorot'\n",
        "  model = MLP(X, y, layers, init_method, rng_key)\n",
        "\n",
        "  # Model Training\n",
        "  model.train(num_epochs = 1000, batch_size = 128)\n",
        "\n",
        "  # Determining the L2 error\n",
        "  opt_params = model.net_params\n",
        "  y_pred = model.predict(opt_params, X_star)\n",
        "\n",
        "  error.append(np.linalg.norm(y_star - y_pred, 2)/np.linalg.norm(y_star, 2))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(rnge, error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrdtcLQR03-F"
      },
      "source": [
        "# **Problem 3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXCRJYbS061t"
      },
      "source": [
        "import jax.numpy as np\n",
        "from jax import random\n",
        "from jax.experimental import stax\n",
        "from jax.experimental.stax import BatchNorm, Conv, Dense, Flatten, Relu, Softmax, MaxPool\n",
        "from jax.experimental import optimizers\n",
        "from jax import jit, grad\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import numpy as onp\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "from functools import partial\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzg_k3c_oKRR"
      },
      "source": [
        "Here we initialize the same one hot encoding for the 10 labels for the CIFAR10 dataset.  Additionally, we create the CIFAR10 Convolutional Neural Network according to the architechture given in the homework."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMwr8AVO1A0l"
      },
      "source": [
        "def one_hot(labels, num_classes, dtype=np.float32):\n",
        "  return np.array(labels[:, None] == np.arange(num_classes), dtype)\n",
        "\n",
        "def cifar_simple_cnn(num_classes):\n",
        "  init_fun, conv_net = stax.serial(Conv(out_chan=6, filter_shape=(5, 5), strides=(1, 1), padding=\"SAME\"),\n",
        "                                   Relu, MaxPool(window_shape=(2,2), strides=(2, 2), padding=\"SAME\"),\n",
        "                                   Conv(out_chan=12, filter_shape=(5, 5), strides=(1, 1), padding=\"SAME\"),\n",
        "                                   Relu, MaxPool(window_shape=(2,2), strides=(2, 2), padding=\"SAME\"),\n",
        "                                   Conv(out_chan=24, filter_shape=(5, 5), strides=(1, 1), padding=\"SAME\"),\n",
        "                                   Relu, MaxPool(window_shape=(2,2), strides=(2, 2), padding=\"SAME\"),\n",
        "                                   Flatten,\n",
        "                                   Dense(384),\n",
        "                                   Dense(120), Dense(84), # Hidden fully connected layers\n",
        "                                   Dense(num_classes), Softmax)\n",
        "  return init_fun, conv_net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdfdScqO34iB"
      },
      "source": [
        "We make some small adjustments to the CNN Classifier given."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwdWReTLtjr6"
      },
      "source": [
        "class CNNclassifier:\n",
        "\n",
        "  \"\"\"\n",
        "  First, we must intialize the network.  We need\n",
        "  to keep track of the number of classes.  We use the\n",
        "  above functions to the initialize the network.  We then\n",
        "  initialize the parameters for the network and instantiate\n",
        "  the Adam optimizer to be used for updating parameters.\n",
        "  Lastly, we set up a counter for iterations, and arrays to\n",
        "  track accuracy and loss.\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize the class\n",
        "  def __init__(self, num_classes, rng_key):\n",
        "    # Store number of classes\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "    # Use stax to set up network initialization and evaluation functions\n",
        "    self.net_init, self.net_apply = cifar_simple_cnn(self.num_classes)\n",
        "    \n",
        "    # Initialize parameters, not committing to a batch shape\n",
        "    _, self.net_params = self.net_init(rng_key, (-1, 3, 32, 32)) # this has changed to the dimensions of the input for CIFAR-10\n",
        "                \n",
        "    # Use optimizers to set optimizer initialization and update functions\n",
        "    self.opt_init, \\\n",
        "    self.opt_update, \\\n",
        "    self.get_params = optimizers.adam(optimizers.exponential_decay(1e-3, \n",
        "                                                                    decay_steps=100, \n",
        "                                                                    decay_rate=0.99))\n",
        "    self.opt_state = self.opt_init(self.net_params)\n",
        "\n",
        "    # Logger\n",
        "    self.itercount = itertools.count()\n",
        "    self.log_acc_train = []\n",
        "    self.log_acc_test = [] \n",
        "    self.train_loss = []\n",
        "  \n",
        "  \"\"\"\n",
        "  We take as input the parameters and the current batch\n",
        "  of data, and output the mean squared error loss.  \n",
        "  \"\"\"\n",
        "\n",
        "  def loss(self, params, batch):\n",
        "      inputs, targets = batch\n",
        "      predictions = self.net_apply(params, inputs)\n",
        "      loss = -np.sum(targets*np.log(predictions + 1e-8))\n",
        "      return loss\n",
        "    \n",
        "  \"\"\"\n",
        "  We take as input the iteration number, the current\n",
        "  state of the Adam optimizer, and the current batch\n",
        "  of data.  We output the updated state of the Adam\n",
        "  optimizer.  The partial decorator indicates we'd\n",
        "  like to prioritize the speed of this function's \n",
        "  compilation and runtime using software and hardware\n",
        "  acceleration.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define a compiled update step\n",
        "  @partial(jit, static_argnums=(0,))\n",
        "  def step(self, i, opt_state, batch):\n",
        "      params = self.get_params(opt_state)\n",
        "      g = grad(self.loss)(params, batch)\n",
        "      return self.opt_update(i, g, opt_state)\n",
        "\n",
        "  \"\"\"\n",
        "  Now, we can compute the accuracy of our model\n",
        "  on a given dataset with it's curent parameters.\n",
        "  This is done using the a one-hot comparison\n",
        "  for the each batch in a dataset/dataloader.  I'm\n",
        "  not sure but I would guess that a dataloader functions\n",
        "  similar to a stream in java.\n",
        "  \"\"\"\n",
        "\n",
        "  def accuracy(self, params, data_loader):\n",
        "    \"\"\" Compute the accuracy for a provided dataloader \"\"\"\n",
        "\n",
        "    acc_total = 0\n",
        "    total_targets = []\n",
        "    total_pred = []\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "        inputs = np.array(inputs)\n",
        "        targets = one_hot(np.array(targets), self.num_classes)\n",
        "        target_labels = np.argmax(targets, axis=1)\n",
        "        predicted_labels = np.argmax(self.net_apply(params, inputs), axis=1)\n",
        "        acc_total += np.sum(predicted_labels == target_labels)\n",
        "\n",
        "        total_targets.extend(target_labels) # We include these logs to fill our confusion matrix\n",
        "        total_pred.extend(predicted_labels)\n",
        "\n",
        "    self.M = confusion_matrix(total_targets, total_pred) # Using the built in confusion matrix\n",
        "\n",
        "    return acc_total/len(data_loader.dataset)\n",
        "  \n",
        "  \"\"\"\n",
        "  Here we train our CNN.  This is done by\n",
        "  performing our accelerated step function\n",
        "  for each batch in a dataset, for a given\n",
        "  number of epochs.  We also compute the training\n",
        "  and test accuracy for classification in \n",
        "  each epoch.  We take as inputs the streams of \n",
        "  training data and test data, as well as the \n",
        "  number of epochs.  At the termination of this\n",
        "  function, our model will be trained and tested,\n",
        "  with data characterizing its accuracy, and the set\n",
        "  of optimal parameters for the CNN.\n",
        "  \"\"\"\n",
        "\n",
        "  # Optimize parameters in a loop\n",
        "  def train(self, train_loader, test_loader, num_epochs = 1000):\n",
        "    for epoch in range(num_epochs):\n",
        "      start_time = time.time()\n",
        "      # Run epoch\n",
        "      for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "          batch = np.array(inputs), one_hot(np.array(targets), self.num_classes)\n",
        "          self.opt_state = self.step(next(self.itercount), self.opt_state, batch)\n",
        "      epoch_time = time.time() - start_time\n",
        "      # Compute training and validation accuracy\n",
        "      self.net_params = self.get_params(self.opt_state)  \n",
        "      loss = self.loss(self.net_params, batch)\n",
        "      train_acc = self.accuracy(self.net_params, train_loader)\n",
        "      test_acc = self.accuracy(self.net_params, test_loader)\n",
        "      self.train_loss.append(loss)\n",
        "      self.log_acc_train.append(train_acc)\n",
        "      self.log_acc_test.append(test_acc)\n",
        "      print(\"Epoch {} | Time: {:0.2f} | Train Acc.: {:0.3f}% | Test Acc.: {:0.3f}%\".format(epoch+1, epoch_time,\n",
        "                                                                  train_acc, test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSAo0kainPlN"
      },
      "source": [
        "Here we import the CIFAR-10 Dataset for training and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqK2zyMGmrF4"
      },
      "source": [
        "# Set the PyTorch Data Loader for the training & test set\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBhgBdqXwukD"
      },
      "source": [
        "num_classes = 10\n",
        "init_key = random.PRNGKey(0)\n",
        "model = CNNclassifier(num_classes, init_key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnJYBlv4w8ZN"
      },
      "source": [
        "model.train(train_loader, test_loader, num_epochs = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFem9UPqsI_D"
      },
      "source": [
        "We use the same confusion matrix plotting function from last week."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LQqlVJesH_6"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWcr_Lt7xEOT"
      },
      "source": [
        "plt.figure(figsize=(24, 8))\n",
        "plt.subplot(1,3,1)\n",
        "plt.plot(model.train_loss)\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training loss')\n",
        "plt.title('Training Loss over Time')\n",
        "plt.subplot(1,3,2)\n",
        "plt.plot(model.log_acc_train, label = 'Training')\n",
        "plt.plot(model.log_acc_test, label = 'Testing')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Testing Accuracy over Time')\n",
        "plt.subplot(1,3,3)\n",
        "cifarClasses = {'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'}\n",
        "plot_confusion_matrix(model.M, cifarClasses, normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPq2ExhJHTmV"
      },
      "source": [
        "# **Problem 4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nypU7XVA200M"
      },
      "source": [
        "import jax.numpy as np\n",
        "from jax import random, vmap, grad, jit\n",
        "from jax.experimental import optimizers\n",
        "from jax.ops import index_update, index\n",
        "\n",
        "import itertools\n",
        "from functools import partial\n",
        "from tqdm import trange\n",
        "import numpy.random as npr\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy.interpolate import griddata\n",
        "from scipy.integrate import odeint"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6piM7VX70Q6_"
      },
      "source": [
        "First, we use the SciPy ODE integration package to generate sequence data for the system of ODE's outlined in the problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKzNWVoLHW53"
      },
      "source": [
        "def dydt(input, t):\n",
        "\n",
        "  x = input[0]\n",
        "  y = input[1]\n",
        "\n",
        "  mu = 5\n",
        "  xdot = mu*(x - (x**3)/3 - y)\n",
        "  ydot = x/mu\n",
        "\n",
        "  return np.array([xdot, ydot])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pHrqQdW2u0Z"
      },
      "source": [
        "# initial conditions\n",
        "y0 = np.array([0.1, 0.5])\n",
        "\n",
        "# time\n",
        "t = np.linspace(0, 60, 2000)\n",
        "\n",
        "# solution to our ODE system for the given sequence\n",
        "output = odeint(dydt, y0, t)\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFYACOhX80gq"
      },
      "source": [
        "plt.figure(figsize=(16,8))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(output)\n",
        "plt.title('State Trajectories')\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(output[:,0], output[:,1])\n",
        "plt.title('System Trajectories')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4I0X7DF6DYp"
      },
      "source": [
        "def create_lags(data, L):\n",
        "    N = data.shape[0] - L\n",
        "    D = data.shape[1]\n",
        "    X = np.zeros((L, N, D))\n",
        "    Y = np.zeros((N, D))\n",
        "    for i in range(0,N):\n",
        "        X = index_update(X, index[:,i,:], data[i:(i+L), :])\n",
        "        Y = index_update(Y, index[i,:], data[i+L, :])\n",
        "    return X, Y\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ1mKiWSWKUP"
      },
      "source": [
        "Now we make some small eedits to the RNN from question 1 in order to make it Long Short-Term.  We must keep track of the gates for this network, so instead of just a reccurent model, our output vector utilizes an output gate, a track of the cell state, an external input gate, and a forget gate.  We perform a series of operations on a larger number of parameters, according to the user input hyperparameters, to produce our output of a given layer.  However, we also maintain the same general architechture for RNNs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkKAaNnPWB8q"
      },
      "source": [
        "class RNN():\n",
        "\n",
        "  \"\"\"\n",
        "  First, we must initialize the RNN.  We take as inputs\n",
        "  the dataset, the number of lags, and the number of hidden\n",
        "  layers.  We first normalize the dataset according to its\n",
        "  mean and standard deviation.  We then create our lagged\n",
        "  normalized dataset using the above function.  We initialize\n",
        "  the network, apply rule, and parameters.  Then, we designate\n",
        "  the Adam optimizer and its state, as well as an array to track\n",
        "  the loss and a counter variable.\n",
        "  \"\"\"\n",
        "  def __init__(self, dataset, num_lags, hidden_dim, rng_key = random.PRNGKey(0)):\n",
        "    # Normalize across data-points dimension\n",
        "    self.mean, self.std = dataset.mean(0), dataset.std(0)\n",
        "    dataset = (dataset - self.mean)/self.std\n",
        "\n",
        "    # Create the lagged normalized trainind data\n",
        "    # X: L x N x D\n",
        "    # Y: N x D\n",
        "    self.X, self.Y = create_lags(dataset, num_lags)\n",
        "    self.X_dim = self.X.shape[-1]\n",
        "    self.Y_dim = self.Y.shape[-1]\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.num_lags = num_lags\n",
        "\n",
        "    # Initialization and evaluation functions\n",
        "    self.net_init, self.net_apply = self.init_RNN()\n",
        "    \n",
        "    # Initialize parameters, not committing to a batch shape\n",
        "    self.net_params = self.net_init(rng_key)\n",
        "                \n",
        "    # Use optimizers to set optimizer initialization and update functions\n",
        "    self.opt_init, \\\n",
        "    self.opt_update, \\\n",
        "    self.get_params = optimizers.adam(1e-3)\n",
        "    self.opt_state = self.opt_init(self.net_params)\n",
        "\n",
        "    # Logger to monitor the loss function\n",
        "    self.loss_log = []\n",
        "    self.itercount = itertools.count()\n",
        "\n",
        "  \"\"\"\n",
        "  Here we initialize the RNN.  We use the glorot\n",
        "  initialization according the number of hidden\n",
        "  layers and the dimension of the lagged normalized\n",
        "  input data X.  We also initialize the biases, weights,\n",
        "  and the output layer.  We output all of these.  We\n",
        "  also define the apply function for our RNN.  This\n",
        "  takes as input the current parameters and some input\n",
        "  data.  We apply a hyperbolic activation function to\n",
        "  (H*W + input*U + b).  This performs the application of \n",
        "  the RNN to input data, according to the activation\n",
        "  functions.\n",
        "  \"\"\"\n",
        "\n",
        "  def init_RNN(self):\n",
        "    # Define init function\n",
        "    def _init(rng_key):\n",
        "\n",
        "        # Define methods for initializing the weights\n",
        "        def glorot_normal(rng_key, size):\n",
        "          in_dim = size[0]\n",
        "          out_dim = size[1]\n",
        "          glorot_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
        "          return glorot_stddev*random.normal(rng_key, (in_dim, out_dim))\n",
        "\n",
        "        # First we define inputs\n",
        "        Uo = glorot_normal(rng_key, (self.X_dim, self.hidden_dim))\n",
        "        Us = glorot_normal(rng_key, (self.X_dim, self.hidden_dim))\n",
        "        Ui = glorot_normal(rng_key, (self.X_dim, self.hidden_dim))\n",
        "        Uf = glorot_normal(rng_key, (self.X_dim, self.hidden_dim))\n",
        "\n",
        "        # Now we can initialize our biases to 0\n",
        "        bo = np.zeros(self.hidden_dim)\n",
        "        bs = np.zeros(self.hidden_dim)\n",
        "        bi = np.zeros(self.hidden_dim)\n",
        "        bf = np.zeros(self.hidden_dim)\n",
        "\n",
        "        # Now we can define the transition dynamics\n",
        "        Wo = np.eye(self.hidden_dim)\n",
        "        Ws = np.eye(self.hidden_dim)\n",
        "        Wi = np.eye(self.hidden_dim)\n",
        "        Wf = np.eye(self.hidden_dim)\n",
        "\n",
        "        # Now we can define our outputs \n",
        "        V = glorot_normal(rng_key, (self.hidden_dim, self.Y_dim))\n",
        "        c = np.zeros(self.Y_dim)\n",
        "\n",
        "        return (Uo, Us, Ui, Uf, bo, bs, bi, bf, Wo, Ws, Wi, Wf, V, c)\n",
        "    # Define apply function\n",
        "    def _apply(params, input):\n",
        "        Uo, Us, Ui, Uf, bo, bs, bi, bf, Wo, Ws, Wi, Wf, V, c = params\n",
        "        H = np.zeros((input.shape[1], self.hidden_dim))\n",
        "        s_t = np.zeros((input.shape[1], self.hidden_dim))\n",
        "        for i in range(self.num_lags):\n",
        "            s_t_tilde = np.tanh(np.matmul(H, Ws) + np.matmul(input[i,:,:], Us) + bs)\n",
        "            f_t = sigmoid(np.matmul(H, Wf) + np.matmul(input[i,:,:], Uf) + bf)\n",
        "            i_t = sigmoid(np.matmul(H, Wi) + np.matmul(input[i,:,:], Ui) + bi)\n",
        "            s_t = (f_t * s_t) + (i_t * s_t_tilde)\n",
        "            o_t = sigmoid(np.matmul(H, Wo) + np.matmul(input[i,:,:], Uo) + bo)\n",
        "            H = o_t * np.tanh(s_t)\n",
        "        H = np.matmul(H, V) + c\n",
        "        return H\n",
        "    return _init, _apply\n",
        "\n",
        "  \"\"\"\n",
        "  Here we calculate the mean squared error loss on an input\n",
        "  of the current parameters and the current batch of data.\n",
        "  \"\"\"\n",
        "\n",
        "  def loss(self, params, batch):\n",
        "    X, y = batch\n",
        "    y_pred = self.net_apply(params, X)\n",
        "    loss = np.mean((y - y_pred)**2)\n",
        "    return loss\n",
        "\n",
        "  \"\"\"\n",
        "  Here we definte the step function of our optimizer.\n",
        "  We take as inputs the iteration number, current Adam\n",
        "  optimizer state, and current batch of data.  We then\n",
        "  output the updated state of the Adam optimizer using\n",
        "  the given data.  We use the partial decorator to indicate\n",
        "  that we will be using this function many times in rapid\n",
        "  succession, so it is best to prioritize the speed of its\n",
        "  compilation and processing using hardware and software\n",
        "  accleration.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define a compiled update step\n",
        "  @partial(jit, static_argnums=(0,))\n",
        "  def step(self, i, opt_state, batch):\n",
        "      params = self.get_params(opt_state)\n",
        "      g = grad(self.loss)(params, batch)\n",
        "      return self.opt_update(i, g, opt_state)\n",
        "\n",
        "  \"\"\"\n",
        "  This function is used to create each batch from a given data set and output it\n",
        "  in a stream.  We take as inputs the dimension of the training data, the number\n",
        "  of batches, and the desired batch size.  We output the training data as the \n",
        "  desired number of batches whose dimensions match the desired size.\n",
        "  \"\"\"\n",
        "\n",
        "  def data_stream(self, n, num_batches, batch_size):\n",
        "    rng = npr.RandomState(0)\n",
        "    while True:\n",
        "      perm = rng.permutation(n)\n",
        "      for i in range(num_batches):\n",
        "        batch_idx = perm[i*batch_size:(i+1)*batch_size]\n",
        "        yield self.X[:, batch_idx, :], self.Y[batch_idx, :]\n",
        "\n",
        "  \"\"\"\n",
        "  This is how we train the RNN.  We perform the accelerated\n",
        "  update rule on each batch in the training data stream, for\n",
        "  a given number of epochs.  We also update our parameters and\n",
        "  loss at each epoch.\n",
        "  \"\"\"\n",
        "\n",
        "  \n",
        "  def train(self, num_epochs = 20000, batch_size = 128):   \n",
        "    n = self.X.shape[1]\n",
        "    num_complete_batches, leftover = divmod(n, batch_size)\n",
        "    num_batches = num_complete_batches + bool(leftover) \n",
        "    batches = self.data_stream(n, num_batches, batch_size)\n",
        "    pbar = trange(num_epochs)\n",
        "    for epoch in pbar:\n",
        "      for _ in range(num_batches):\n",
        "        batch = next(batches)\n",
        "        self.opt_state = self.step(next(self.itercount), self.opt_state, batch)\n",
        "      self.net_params = self.get_params(self.opt_state)\n",
        "      loss_value = self.loss(self.net_params, batch)\n",
        "      self.loss_log.append(loss_value)\n",
        "      pbar.set_postfix({'Loss': loss_value})\n",
        "\n",
        "  \"\"\"\n",
        "  Finally, we have a function that can take in the current\n",
        "  parameters and some input test data to make a prediction\n",
        "  using the RNN.  We again use the partial decorator to speed\n",
        "  up the testing of our RNN.\n",
        "  \"\"\"\n",
        "\n",
        "  @partial(jit, static_argnums=(0,))\n",
        "  def predict(self, params, inputs):\n",
        "    Y_pred = self.net_apply(params, inputs)\n",
        "    return Y_pred"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwHkuv7ecrW_"
      },
      "source": [
        "rng_key = random.PRNGKey(0)\n",
        "noise = 0.0\n",
        "\n",
        "dataset = output\n",
        "dataset = dataset + dataset.std(0)*noise*random.normal(rng_key, dataset.shape)\n",
        "\n",
        "# Use 2/3 of all data as training Data\n",
        "train_size = int(len(dataset) * (2.0/3.0))\n",
        "train_data = dataset[0:train_size,:]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtU1oQYWdqHa"
      },
      "source": [
        "# Model creation\n",
        "num_lags = 8\n",
        "hidden_dim = 20\n",
        "model = RNN(train_data, num_lags, hidden_dim, rng_key)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTce1A_Afquw"
      },
      "source": [
        "model.train(num_epochs = 100, batch_size = 128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsYRs-Jwi5K4"
      },
      "source": [
        "opt_params = model.net_params\n",
        "# One-step ahead prediction (normalized)\n",
        "N, D = dataset.shape\n",
        "pred = np.zeros((N-num_lags, D))\n",
        "X_tmp =  model.X[:,0:1,:]\n",
        "\n",
        "for i in trange(N-num_lags):\n",
        "    pred = index_update(pred, index[i:i+1], model.net_apply(opt_params, X_tmp))\n",
        "    X_tmp = index_update(X_tmp, index[:-1,:,:], X_tmp[1:,:,:])\n",
        "    X_tmp = index_update(X_tmp, index[-1,:,:], pred[i])\n",
        "# De-normalize predictions\n",
        "pred = pred*model.std + model.mean\n",
        "error = np.linalg.norm(dataset[num_lags:] - pred, 2)/np.linalg.norm(dataset[num_lags:], 2)\n",
        "print('Relative L2 prediction error: %e' % (error))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFzBQ3-vqxnv"
      },
      "source": [
        "As we see from the above output, the L2 error is fairly small.\n",
        "\n",
        "Additionally, below, the graph of our predicted vs actual values is shown.  The prediction appears to be fairly accurate in this regard as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZflWG2djCia"
      },
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.plot(dataset[num_lags:], 'b-', linewidth = 2, label = \"Exact\")\n",
        "plt.plot(pred, 'r--', linewidth = 3, label = \"Prediction\")\n",
        "# plt.plot(X.shape[1]*np.ones((2,1)), np.linspace(-1.75,1.75,2), 'k--', linewidth=2)\n",
        "plt.axvline(train_size)\n",
        "plt.axis('tight')\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$y_t, outputs$')\n",
        "plt.legend(loc='lower left')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}